{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "import src.dictionary as dictionary\n",
    "import src.processing as processing\n",
    "import src.network as network\n",
    "import pickle\n",
    "\n",
    "# Update python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_grams = 3\n",
    "sliding_window_size = 15\n",
    "embedding_size = 256\n",
    "use_pretrained_embeddings = False\n",
    "reg_alpha = 1e-4\n",
    "lr = 1e-3\n",
    "dropout_prob = 0.6\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "\n",
    "assert(n_grams % 2 == 1 and sliding_window_size % 2 == 1)\n",
    "\n",
    "# Save parameters, so that when restore model for testing, I have them\n",
    "parameters = {\"n_grams\" : n_grams, \n",
    "              \"sliding_window_size\" : sliding_window_size, \n",
    "              \"embedding_size\" : embedding_size\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">P0A8Q6\n",
      "MGKTNDWLDFDQLAEEKVRDALKPPSMYKVILVNDDYTPMEFVIDVLQKFFSYDVERATQLMLAVHYQGKAICGVFTAEVAETKVAMVNKYARENEHPLLCTLEKA\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for prediction\n",
    "protein_names, sequences, labels = [], [], []\n",
    "\n",
    "\n",
    "'''\n",
    "    Labels:\n",
    "        \"+\" stands for \"binding protein\" => 1\n",
    "        \"-\" stands for \"non-binding\" => 0\n",
    "'''\n",
    "def convert_label(label_string):\n",
    " \n",
    "    if label_string == \"+\":\n",
    "        return 1\n",
    "    elif label_string == \"-\":\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"Should not enter here\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "# Open file containing dataset    \n",
    "with open('./dataset/ppi_data.fasta') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        \n",
    "        if i % 3 == 0:\n",
    "            protein_names.append(lines[i])\n",
    "        elif i % 3 == 1:\n",
    "            sequences.append(lines[i])\n",
    "        elif i % 3 == 2:\n",
    "            labels.append([convert_label(letter) for letter in lines[i]])\n",
    "            \n",
    "protein_names = np.array(protein_names)\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "assert(protein_names.shape[0] == sequences.shape[0] == labels.shape[0])\n",
    "\n",
    "print(protein_names[0])\n",
    "print(sequences[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences = sequences[:24]\n",
    "#labels = labels[:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 432\n",
      "Validation samples: 108\n",
      "\n",
      "MPQSFTSIARIGDYILKSPVLSKLCVPVANQFINLAGYKKLGLKFDDLIAEENPIMQTALRRLPEDESYARAYRIIRAHQTELTHHLLPRNEWIKAQEDVPYLLPYILEAEAAAKEKDELDNIEVSK\n",
      "[0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Split percentage of training and validation\n",
    "split_percentage = 0.8\n",
    "\n",
    "# Count how many samples into training dataset\n",
    "total_dataset = len(sequences)\n",
    "train_dataset = int(total_dataset * split_percentage)\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(97)\n",
    "indices = list(range(total_dataset))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Train dataset\n",
    "sequences_train = sequences[indices[:train_dataset]]\n",
    "labels_train = labels[indices[:train_dataset]]\n",
    "\n",
    "# Validation dataset\n",
    "sequences_val = sequences[indices[train_dataset:]]\n",
    "labels_val = labels[indices[train_dataset:]]\n",
    "\n",
    "# Shapes\n",
    "print(\"Training samples: \" + str(sequences_train.shape[0]))\n",
    "print(\"Validation samples: \" + str(sequences_val.shape[0]) + \"\\n\")\n",
    "\n",
    "# Reset seed for randomness\n",
    "np.random.seed()\n",
    "\n",
    "# Sample\n",
    "print(sequences_train[0])\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69953, 13)\n",
      "(15268, 13)\n",
      "['MPQ' 'PQS' 'QSF' 'SFT' 'FTS' 'TSI' 'SIA' 'IAR' 'ARI' 'RIG' 'IGD' 'GDY'\n",
      " 'DYI']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = processing.create_input(sequences_train, labels_train, sliding_window_size, n_grams)\n",
    "X_val_proc, Y_val_proc = processing.create_input(sequences_val, labels_val, sliding_window_size, n_grams)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val_proc.shape)\n",
    "\n",
    "print(X_train[0])\n",
    "print(Y_train[0])\n",
    "\n",
    "# Save dump for parameters\n",
    "parameters[\"timestamps\"] = X_train.shape[1]\n",
    "with open('./dumps/parameters.pickle', 'wb') as handle:\n",
    "    pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 19105 | Negative: 50848\n",
      "Final training data shape: (101696, 13)\n"
     ]
    }
   ],
   "source": [
    "# Find indices with positive and negative labels\n",
    "X_train_pos = np.array([i for i in range(len(X_train)) if Y_train[i] == 1])\n",
    "X_train_neg = np.array([i for i in range(len(X_train)) if Y_train[i] == 0])\n",
    "\n",
    "# Print how many positive and negative labels => I want same number of labels for each class during training\n",
    "print(\"Positive: \" + str(X_train_pos.shape[0]) + \" | Negative: \" + str(X_train_neg.shape[0]))\n",
    "\n",
    "# Get indices from X_train_pos\n",
    "np.random.seed(97)\n",
    "X_train_pos_indices = np.random.choice(len(X_train_pos), len(X_train_neg), replace=True)\n",
    "X_train_selected = X_train_pos[X_train_pos_indices]\n",
    "\n",
    "# Final X_train data\n",
    "X_train = np.concatenate((X_train[X_train_selected], X_train[X_train_neg]), axis=0)\n",
    "Y_train = np.concatenate((Y_train[X_train_selected], Y_train[X_train_neg]), axis=0)\n",
    "\n",
    "# Check that labels 1 and 0 are equal\n",
    "assert(len(np.array([i for i in range(len(X_train)) if Y_train[i] == 1])) == \n",
    "    len(np.array([i for i in range(len(X_train)) if Y_train[i] == 0])))\n",
    "\n",
    "print(\"Final training data shape: \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer words: 7707\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary of n-grams\n",
    "vocab = dictionary.LanguageDictionary(X_train)\n",
    "\n",
    "# Load embeddings trained with Protovec\n",
    "trained_embeddings = processing.loadEmbeddings(vocab, \"./train_embeddings/models/ngram-\" + str(n_grams) + \".model\")\n",
    "\n",
    "assert(len(vocab.index_to_word) == len(vocab.word_to_index) == len(trained_embeddings))\n",
    "print(\"Embedding layer words: \" + str(len(trained_embeddings)))\n",
    "\n",
    "# Save vocabulary locally with pickle dump\n",
    "with open('./dumps/vocab.pickle', 'wb') as handle:\n",
    "    pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101696, 13)\n",
      "(14008, 13)\n"
     ]
    }
   ],
   "source": [
    "# Map grams to indices for the embedding matrix and remove samples where unknown words\n",
    "X_train = np.array([vocab.text_to_indices(tmp) for tmp in X_train])\n",
    "print(X_train.shape)\n",
    "\n",
    "# Prepare validation data\n",
    "X_val, Y_val = [], []\n",
    "for i in range(len(X_val_proc)):\n",
    "    \n",
    "    tmp = vocab.text_to_indices(X_val_proc[i])\n",
    "    if not None in tmp:\n",
    "        X_val.append(tmp)\n",
    "        Y_val.append(Y_val_proc[i])\n",
    "        \n",
    "X_val = np.array(X_val)\n",
    "Y_val = np.array(Y_val)\n",
    "\n",
    "assert(len(X_val) == len(Y_val))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "tensor_X = tf.placeholder(tf.int32, (None, X_train.shape[1]), 'inputs')\n",
    "tensor_Y = tf.placeholder(tf.int32, (None), 'output')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "\n",
    "# Create graph for the network\n",
    "if use_pretrained_embeddings:\n",
    "    assert(len(trained_embeddings[0]) == embedding_size)\n",
    "else:\n",
    "    trained_embeddings = None\n",
    "\n",
    "logits = network.create_network(tensor_X, tensor_Y, keep_prob, vocab, embedding_size, trained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tensor_Y)\n",
    "meaned = tf.reduce_mean(ce)\n",
    "\n",
    "trainable_vars = tf.trainable_variables()\n",
    "l2_reg = tf.reduce_sum([tf.nn.l2_loss(var) for var in trainable_vars])\n",
    "loss = meaned + reg_alpha * l2_reg\n",
    "\n",
    "# Using Adam (Adaptive learning rate + momentum) for the update of the weights of the network\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=1))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, tensor_Y))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations per epoch: 1589\n",
      "SAVE | Val loss: 26.927614, accuracy: 0.5357653\n",
      "SAVE | Val loss: 21.514845, accuracy: 0.53997713\n",
      "SAVE | Val loss: 17.033527, accuracy: 0.5409052\n",
      "SAVE | Val loss: 10.658874, accuracy: 0.54575956\n",
      "SAVE | Val loss: 8.610101, accuracy: 0.559823\n",
      "SAVE | Val loss: 7.0800824, accuracy: 0.5875928\n",
      "SAVE | Val loss: 3.9299293, accuracy: 0.6104369\n",
      "SAVE | Val loss: 2.4537804, accuracy: 0.6121502\n",
      "SAVE | Val loss: 2.303782, accuracy: 0.62064534\n",
      "SAVE | Val loss: 1.7967415, accuracy: 0.6226442\n",
      "SAVE | Val loss: 1.7373092, accuracy: 0.622787\n",
      "SAVE | Val loss: 1.6823735, accuracy: 0.6425614\n",
      "SAVE | Val loss: 1.5811579, accuracy: 0.6448458\n",
      "SAVE | Val loss: 1.9323804, accuracy: 0.6477013\n",
      "SAVE | Val loss: 1.9318738, accuracy: 0.6484152\n",
      "SAVE | Val loss: 1.9679573, accuracy: 0.64855796\n",
      "SAVE | Val loss: 1.8474904, accuracy: 0.6489863\n",
      "SAVE | Val loss: 1.9610058, accuracy: 0.64927185\n",
      "SAVE | Val loss: 1.933443, accuracy: 0.6501999\n",
      "SAVE | Val loss: 1.9238939, accuracy: 0.65969443\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Training data variables\n",
    "iterations_training = max((len(X_train) // batch_size), 1)\n",
    "print(\"Training iterations per epoch: \" + str(iterations_training))\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "iterations_validation = max((len(X_val) // batch_size), 1)\n",
    "\n",
    "# Perform each epoch, shuffle training dataset\n",
    "indices = list(range(len(X_train)))\n",
    "consecutive_validation = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables in the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle data (with random seed for debug) to not train the network always with the same order\n",
    "        np.random.seed(97)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        Y_train = Y_train[indices]\n",
    "        \n",
    "        # Vector accumulating accuracy and loss during one epoch\n",
    "        total_accuracies, total_losses = [], []\n",
    "\n",
    "        # Iterate over mini-batches\n",
    "        for j in range(iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size \n",
    "            \n",
    "            # If last batch, take also elements that are less than batch_size\n",
    "            if j == (iterations_training - 1):\n",
    "                end_index += (batch_size - 1)\n",
    "\n",
    "            _, avg_accuracy, avg_loss = sess.run([optimizer, accuracy, loss], feed_dict={\n",
    "                                                        tensor_X: X_train[start_index:end_index],\n",
    "                                                        tensor_Y: Y_train[start_index:end_index],\n",
    "                                                        keep_prob: dropout_prob})\n",
    "            # Add values for this mini-batch iterations\n",
    "            total_losses.append(avg_loss) \n",
    "            total_accuracies.append(avg_accuracy)\n",
    "\n",
    "            #if (j+1) % 100 == 0:\n",
    "            #    print(\"Loss: \" + str(avg_loss) + \", Accuracy: \" + str(avg_accuracy))\n",
    "                    \n",
    "            # Statistics on validation set\n",
    "            if (j+1) % 100 == 0:  \n",
    "                avg_accuracy, avg_loss, pred = sess.run([accuracy, loss, predictions], feed_dict={ \n",
    "                                                                                    tensor_X: X_val,\n",
    "                                                                                    tensor_Y: Y_val,\n",
    "                                                                                    keep_prob: 1.0 })\n",
    "                #avg_accuracy = precision_score(Y_val, pred)\n",
    "                \n",
    "                # Save model if validation accuracy better\n",
    "                if avg_accuracy > max_val_acc:\n",
    "                    consecutive_validation_without_saving = 0\n",
    "                    max_val_acc = avg_accuracy\n",
    "                    print(\"SAVE | Val loss: \" + str(avg_loss) + \", accuracy: \" + str(avg_accuracy))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")\n",
    "                    consecutive_validation = 0\n",
    "                else:\n",
    "                    consecutive_validation += 1\n",
    "            \n",
    "        if consecutive_validation >= 35:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        # Epoch statistics\n",
    "        #print(\"Epoch: \" + str(i) + \", Loss: \" + str(np.mean(np.array(total_losses))) + \n",
    "        #      \", Acc: \" + str(np.mean(np.array(total_accuracies))) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"./checkpoints/model.ckpt\") \n",
    "    \n",
    "    avg_accuracy, avg_loss, pred = sess.run([accuracy, loss, predictions], feed_dict={\n",
    "                                            tensor_X: X_val,\n",
    "                                            tensor_Y: Y_val,\n",
    "                                            keep_prob: 1.0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0\n",
      " 0 0]\n",
      "[1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0]\n",
      "\n",
      "Accuracy :0.6596944603083952\n",
      "Precision :0.37325227963525837\n",
      "Recall :0.14114942528735633\n",
      "AUC :0.5171992725939785\n"
     ]
    }
   ],
   "source": [
    "print(Y_val[-150:])\n",
    "print(pred[-150:])\n",
    "\n",
    "\n",
    "print(\"\\nAccuracy :\" + str(accuracy_score(Y_val, pred)))\n",
    "print(\"Precision :\" + str(precision_score(Y_val, pred)))\n",
    "print(\"Recall :\" + str(recall_score(Y_val, pred)))\n",
    "print(\"AUC :\" + str(roc_auc_score(Y_val, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
