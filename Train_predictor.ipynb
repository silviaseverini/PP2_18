{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "import src.dictionary as dictionary\n",
    "import src.processing as processing\n",
    "import src.network as network\n",
    "import pickle\n",
    "\n",
    "# Update python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_grams = 3\n",
    "sliding_window_size = 15\n",
    "embedding_size = 256\n",
    "use_pretrained_embeddings = False\n",
    "reg_alpha = 1e-4\n",
    "lr = 1e-3\n",
    "dropout_prob = 0.6\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "\n",
    "assert(n_grams % 2 == 1 and sliding_window_size % 2 == 1)\n",
    "\n",
    "# Save parameters, so that when restore model for testing, I have them\n",
    "parameters = {\"n_grams\" : n_grams, \n",
    "              \"sliding_window_size\" : sliding_window_size, \n",
    "              \"embedding_size\" : embedding_size\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">P0A8Q6\n",
      "MGKTNDWLDFDQLAEEKVRDALKPPSMYKVILVNDDYTPMEFVIDVLQKFFSYDVERATQLMLAVHYQGKAICGVFTAEVAETKVAMVNKYARENEHPLLCTLEKA\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for prediction\n",
    "protein_names, sequences, labels = [], [], []\n",
    "\n",
    "\n",
    "'''\n",
    "    Labels:\n",
    "        \"+\" stands for \"binding protein\" => 1\n",
    "        \"-\" stands for \"non-binding\" => 0\n",
    "'''\n",
    "def convert_label(label_string):\n",
    " \n",
    "    if label_string == \"+\":\n",
    "        return 1\n",
    "    elif label_string == \"-\":\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"Should not enter here\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "# Open file containing dataset    \n",
    "with open('./dataset/ppi_data.fasta') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        \n",
    "        if i % 3 == 0:\n",
    "            protein_names.append(lines[i])\n",
    "        elif i % 3 == 1:\n",
    "            sequences.append(lines[i])\n",
    "        elif i % 3 == 2:\n",
    "            labels.append([convert_label(letter) for letter in lines[i]])\n",
    "            \n",
    "protein_names = np.array(protein_names)\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "assert(protein_names.shape[0] == sequences.shape[0] == labels.shape[0])\n",
    "\n",
    "print(protein_names[0])\n",
    "print(sequences[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences = sequences[:24]\n",
    "#labels = labels[:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 432\n",
      "Validation samples: 108\n",
      "\n",
      "MPQSFTSIARIGDYILKSPVLSKLCVPVANQFINLAGYKKLGLKFDDLIAEENPIMQTALRRLPEDESYARAYRIIRAHQTELTHHLLPRNEWIKAQEDVPYLLPYILEAEAAAKEKDELDNIEVSK\n",
      "[0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Split percentage of training and validation\n",
    "split_percentage = 0.8\n",
    "\n",
    "# Count how many samples into training dataset\n",
    "total_dataset = len(sequences)\n",
    "train_dataset = int(total_dataset * split_percentage)\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(97)\n",
    "indices = list(range(total_dataset))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Train dataset\n",
    "sequences_train = sequences[indices[:train_dataset]]\n",
    "labels_train = labels[indices[:train_dataset]]\n",
    "\n",
    "# Validation dataset\n",
    "sequences_val = sequences[indices[train_dataset:]]\n",
    "labels_val = labels[indices[train_dataset:]]\n",
    "\n",
    "# Shapes\n",
    "print(\"Training samples: \" + str(sequences_train.shape[0]))\n",
    "print(\"Validation samples: \" + str(sequences_val.shape[0]) + \"\\n\")\n",
    "\n",
    "# Reset seed for randomness\n",
    "np.random.seed()\n",
    "\n",
    "# Sample\n",
    "print(sequences_train[0])\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69953, 13)\n",
      "(15268, 13)\n",
      "['MPQ' 'PQS' 'QSF' 'SFT' 'FTS' 'TSI' 'SIA' 'IAR' 'ARI' 'RIG' 'IGD' 'GDY'\n",
      " 'DYI']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = processing.create_input(sequences_train, labels_train, sliding_window_size, n_grams)\n",
    "X_val_proc, Y_val_proc = processing.create_input(sequences_val, labels_val, sliding_window_size, n_grams)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val_proc.shape)\n",
    "\n",
    "print(X_train[0])\n",
    "print(Y_train[0])\n",
    "\n",
    "# Save dump for parameters\n",
    "parameters[\"timestamps\"] = X_train.shape[1]\n",
    "with open('./dumps/parameters.pickle', 'wb') as handle:\n",
    "    pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 19105 | Negative: 50848\n",
      "Final training data shape: (101696, 13)\n"
     ]
    }
   ],
   "source": [
    "# Find indices with positive and negative labels\n",
    "X_train_pos = np.array([i for i in range(len(X_train)) if Y_train[i] == 1])\n",
    "X_train_neg = np.array([i for i in range(len(X_train)) if Y_train[i] == 0])\n",
    "\n",
    "# Print how many positive and negative labels => I want same number of labels for each class during training\n",
    "print(\"Positive: \" + str(X_train_pos.shape[0]) + \" | Negative: \" + str(X_train_neg.shape[0]))\n",
    "\n",
    "# Get indices from X_train_pos\n",
    "np.random.seed(97)\n",
    "X_train_pos_indices = np.random.choice(len(X_train_pos), len(X_train_neg), replace=True)\n",
    "X_train_selected = X_train_pos[X_train_pos_indices]\n",
    "\n",
    "# Final X_train data\n",
    "X_train = np.concatenate((X_train[X_train_selected], X_train[X_train_neg]), axis=0)\n",
    "Y_train = np.concatenate((Y_train[X_train_selected], Y_train[X_train_neg]), axis=0)\n",
    "\n",
    "# Check that labels 1 and 0 are equal\n",
    "assert(len(np.array([i for i in range(len(X_train)) if Y_train[i] == 1])) == \n",
    "    len(np.array([i for i in range(len(X_train)) if Y_train[i] == 0])))\n",
    "\n",
    "print(\"Final training data shape: \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer words: 7707\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary of n-grams\n",
    "vocab = dictionary.LanguageDictionary(X_train)\n",
    "\n",
    "# Load embeddings trained with Protovec\n",
    "trained_embeddings = processing.loadEmbeddings(vocab, \"./train_embeddings/models/ngram-\" + str(n_grams) + \".model\")\n",
    "\n",
    "assert(len(vocab.index_to_word) == len(vocab.word_to_index) == len(trained_embeddings))\n",
    "print(\"Embedding layer words: \" + str(len(trained_embeddings)))\n",
    "\n",
    "# Save vocabulary locally with pickle dump\n",
    "with open('./dumps/vocab.pickle', 'wb') as handle:\n",
    "    pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101696, 13)\n",
      "(14008, 13)\n"
     ]
    }
   ],
   "source": [
    "# Map grams to indices for the embedding matrix and remove samples where unknown words\n",
    "X_train = np.array([vocab.text_to_indices(tmp) for tmp in X_train])\n",
    "print(X_train.shape)\n",
    "\n",
    "# Prepare validation data\n",
    "X_val, Y_val = [], []\n",
    "for i in range(len(X_val_proc)):\n",
    "    \n",
    "    tmp = vocab.text_to_indices(X_val_proc[i])\n",
    "    if not None in tmp:\n",
    "        X_val.append(tmp)\n",
    "        Y_val.append(Y_val_proc[i])\n",
    "        \n",
    "X_val = np.array(X_val)\n",
    "Y_val = np.array(Y_val)\n",
    "\n",
    "assert(len(X_val) == len(Y_val))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "tensor_X = tf.placeholder(tf.int32, (None, X_train.shape[1]), 'inputs')\n",
    "tensor_Y = tf.placeholder(tf.int32, (None), 'output')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "\n",
    "# Create graph for the network\n",
    "if use_pretrained_embeddings:\n",
    "    assert(len(trained_embeddings[0]) == embedding_size)\n",
    "else:\n",
    "    trained_embeddings = None\n",
    "\n",
    "logits = network.create_network(tensor_X, tensor_Y, keep_prob, vocab, embedding_size, trained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tensor_Y)\n",
    "meaned = tf.reduce_mean(ce)\n",
    "\n",
    "trainable_vars = tf.trainable_variables()\n",
    "l2_reg = tf.reduce_sum([tf.nn.l2_loss(var) for var in trainable_vars])\n",
    "loss = meaned + reg_alpha * l2_reg\n",
    "\n",
    "# Using Adam (Adaptive learning rate + momentum) for the update of the weights of the network\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=1))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, tensor_Y))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations per epoch: 1589\n",
      "SAVE | Val loss: 26.84327, accuracy: 0.56788975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9f4d62aa9ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                                                         \u001b[0mtensor_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                                         \u001b[0mtensor_Y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                                         keep_prob: dropout_prob})\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Add values for this mini-batch iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtotal_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training data variables\n",
    "iterations_training = max((len(X_train) // batch_size), 1)\n",
    "print(\"Training iterations per epoch: \" + str(iterations_training))\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "iterations_validation = max((len(X_val) // batch_size), 1)\n",
    "\n",
    "# Perform each epoch, shuffle training dataset\n",
    "indices = list(range(len(X_train)))\n",
    "consecutive_validation = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables in the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle data (with random seed for debug) to not train the network always with the same order\n",
    "        np.random.seed(97)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        Y_train = Y_train[indices]\n",
    "        \n",
    "        # Vector accumulating accuracy and loss during one epoch\n",
    "        total_accuracies, total_losses = [], []\n",
    "\n",
    "        # Iterate over mini-batches\n",
    "        for j in range(iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size \n",
    "            \n",
    "            # If last batch, take also elements that are less than batch_size\n",
    "            if j == (iterations_training - 1):\n",
    "                end_index += (batch_size - 1)\n",
    "\n",
    "            _, avg_accuracy, avg_loss = sess.run([optimizer, accuracy, loss], feed_dict={\n",
    "                                                        tensor_X: X_train[start_index:end_index],\n",
    "                                                        tensor_Y: Y_train[start_index:end_index],\n",
    "                                                        keep_prob: dropout_prob})\n",
    "            # Add values for this mini-batch iterations\n",
    "            total_losses.append(avg_loss) \n",
    "            total_accuracies.append(avg_accuracy)\n",
    "\n",
    "            #if (j+1) % 100 == 0:\n",
    "            #    print(\"Loss: \" + str(avg_loss) + \", Accuracy: \" + str(avg_accuracy))\n",
    "                    \n",
    "            # Statistics on validation set\n",
    "            if (j+1) % 100 == 0:  \n",
    "                avg_accuracy, avg_loss, pred = sess.run([accuracy, loss, predictions], feed_dict={ \n",
    "                                                                                    tensor_X: X_val,\n",
    "                                                                                    tensor_Y: Y_val,\n",
    "                                                                                    keep_prob: 1.0 })\n",
    "                #avg_accuracy = precision_score(Y_val, pred)\n",
    "                \n",
    "                # Save model if validation accuracy better\n",
    "                if avg_accuracy > max_val_acc:\n",
    "                    consecutive_validation_without_saving = 0\n",
    "                    max_val_acc = avg_accuracy\n",
    "                    print(\"SAVE | Val loss: \" + str(avg_loss) + \", accuracy: \" + str(avg_accuracy))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")\n",
    "                    consecutive_validation = 0\n",
    "                else:\n",
    "                    consecutive_validation += 1\n",
    "            \n",
    "        if consecutive_validation >= 35:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        # Epoch statistics\n",
    "        #print(\"Epoch: \" + str(i) + \", Loss: \" + str(np.mean(np.array(total_losses))) + \n",
    "        #      \", Acc: \" + str(np.mean(np.array(total_accuracies))) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"./checkpoints/model.ckpt\") \n",
    "    \n",
    "    avg_accuracy, avg_loss, pred = sess.run([accuracy, loss, predictions], feed_dict={\n",
    "                                            tensor_X: X_val,\n",
    "                                            tensor_Y: Y_val,\n",
    "                                            keep_prob: 1.0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0\n",
      " 0 0]\n",
      "[0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 1]\n",
      "\n",
      "Accuracy :0.5678897772701313\n",
      "Precision :0.3177049882252194\n",
      "Recall :0.3411494252873563\n",
      "AUC :0.5055819605210855\n"
     ]
    }
   ],
   "source": [
    "print(Y_val[-150:])\n",
    "print(pred[-150:])\n",
    "\n",
    "\n",
    "print(\"\\nAccuracy :\" + str(accuracy_score(Y_val, pred)))\n",
    "print(\"Precision :\" + str(precision_score(Y_val, pred)))\n",
    "print(\"Recall :\" + str(recall_score(Y_val, pred)))\n",
    "print(\"AUC :\" + str(roc_auc_score(Y_val, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
