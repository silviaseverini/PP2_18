{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import dictionary\n",
    "\n",
    "# Update python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 3\n",
    "sliding_window_size = 9\n",
    "embedding_size = 512\n",
    "lr = 1e-4\n",
    "batch_size = 64\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">P0A8Q6\n",
      "MGKTNDWLDFDQLAEEKVRDALKPPSMYKVILVNDDYTPMEFVIDVLQKFFSYDVERATQLMLAVHYQGKAICGVFTAEVAETKVAMVNKYARENEHPLLCTLEKA\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for prediction\n",
    "protein_names, sequences, labels = [], [], []\n",
    "\n",
    "\n",
    "'''\n",
    "    Labels:\n",
    "        \"+\" stands for \"binding protein\" => 1\n",
    "        \"-\" stands for \"non-binding\" => 0\n",
    "'''\n",
    "def convert_label(label_string):\n",
    " \n",
    "    if label_string == \"+\":\n",
    "        return 1\n",
    "    elif label_string == \"-\":\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"Should not enter here\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "# Open file containing dataset    \n",
    "with open('./ppi_data.fasta') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        \n",
    "        if i % 3 == 0:\n",
    "            protein_names.append(lines[i])\n",
    "        elif i % 3 == 1:\n",
    "            sequences.append(lines[i])\n",
    "        elif i % 3 == 2:\n",
    "            labels.append([convert_label(letter) for letter in lines[i]])\n",
    "            \n",
    "protein_names = np.array(protein_names)\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "assert(protein_names.shape[0] == sequences.shape[0] == labels.shape[0])\n",
    "\n",
    "print(protein_names[0])\n",
    "print(sequences[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88461, 7)\n",
      "(88461,)\n"
     ]
    }
   ],
   "source": [
    "amino_acids, bindings = [], []\n",
    "\n",
    "# Iterates over all proteins in dataset\n",
    "for i in range(len(sequences)):\n",
    "\n",
    "    # Loop over sequence\n",
    "    for j in range(0,len(sequences[i]) - sliding_window_size + 1):\n",
    "        sub_sequence = sequences[i][j:j+sliding_window_size]\n",
    "\n",
    "        tmp = []\n",
    "        for k in range(0, sliding_window_size - n_grams + 1):\n",
    "            tmp.append(sub_sequence[k:k+n_grams])\n",
    "    \n",
    "        amino_acids.append(tmp)\n",
    "        bindings.append(labels[i][j+(sliding_window_size//2)]) \n",
    "        \n",
    "amino_acids, bindings = np.array(amino_acids), np.array(bindings)\n",
    "print(amino_acids.shape)\n",
    "print(bindings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 70768\n",
      "Validation samples: 17693\n",
      "\n",
      "MGKTNDWLDFDQLAEEKVRDALKPPSMYKVILVNDDYTPMEFVIDVLQKFFSYDVERATQLMLAVHYQGKAICGVFTAEVAETKVAMVNKYARENEHPLLCTLEKA\n",
      "['DDG' 'DGS' 'GSF' 'SFE' 'FEI' 'EIE' 'IEV']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Split percentage of training and validation\n",
    "split_percentage = 0.8\n",
    "\n",
    "# Count how many samples into training dataset\n",
    "total_dataset = len(amino_acids)\n",
    "train_dataset = int(total_dataset * split_percentage)\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(145)\n",
    "indices = list(range(total_dataset))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Train dataset\n",
    "X_train = amino_acids[indices[:train_dataset]]\n",
    "y_train = bindings[indices[:train_dataset]]\n",
    "\n",
    "# Validation dataset\n",
    "X_val = amino_acids[indices[train_dataset:]]\n",
    "y_val = bindings[indices[train_dataset:]]\n",
    "\n",
    "# Shapes\n",
    "print(\"Training samples: \" + str(X_train.shape[0]))\n",
    "print(\"Validation samples: \" + str(X_val.shape[0]) + \"\\n\")\n",
    "\n",
    "# Reset seed for randomness\n",
    "np.random.seed()\n",
    "\n",
    "# Sample\n",
    "print(sequences[0])\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70768, 7)\n",
      "(70768,)\n",
      "(17693, 7)\n",
      "(17693,)\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary of n-grams\n",
    "vocab = dictionary.LanguageDictionary(X_train)\n",
    "\n",
    "# Map grams to indices for the embedding matrix\n",
    "X_train_mapped = np.array([vocab.text_to_indices(tmp) for tmp in X_train])\n",
    "X_val_mapped = np.array([vocab.text_to_indices(tmp) for tmp in X_val if not vocab.text_to_indices(tmp) is None])\n",
    "\n",
    "# Shapes\n",
    "print(X_train_mapped.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val_mapped.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17693, 7)\n",
      "(17693,)\n"
     ]
    }
   ],
   "source": [
    "X_final_val = []\n",
    "y_final_val = []\n",
    "\n",
    "for i in range(len(X_val_mapped)):\n",
    "\n",
    "    is_none = False\n",
    "    for j in range(len(X_val_mapped[i])):\n",
    "\n",
    "        if X_val_mapped[i][j] is None:\n",
    "            is_none = True\n",
    "    \n",
    "    if not is_none:\n",
    "        X_final_val.append(X_val_mapped[i])\n",
    "        y_final_val.append(y_val[i])    \n",
    "    \n",
    "X_final_val = np.array(X_final_val)\n",
    "y_final_val = np.array(y_final_val)\n",
    "\n",
    "print(X_final_val.shape)\n",
    "print(y_final_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape, name=None):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "\n",
    "def new_biases(length, name=None):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[length]), name=name)\n",
    "\n",
    "\n",
    "def embedding_layer(input_x, vocabulary_size, embedding_size):\n",
    "    init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "    embeddings = tf.Variable(init_embeds)\n",
    "    layer = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "def create_network(X, Y, vocabulary, embedding_size, verbose):\n",
    "    \n",
    "    embedding = embedding_layer(X, len(vocabulary.index_to_word), embedding_size)\n",
    "    \n",
    "    \n",
    "    lstm_fw_cell = tf.contrib.rnn.LSTMCell(128, forget_bias=1.0)\n",
    "    lstm_bw_cell = tf.contrib.rnn.LSTMCell(128, forget_bias=1.0)\n",
    "    (outputs_fw, outputs_bw), last_states = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedding, dtype=tf.float32)\n",
    "\n",
    "    # Get last output of LSTM\n",
    "    outputs_fw = tf.transpose(outputs_fw, [1, 0, 2])\n",
    "    last_output_fw = tf.gather(outputs_fw, int(outputs_fw.get_shape()[0]) - 1)\n",
    "\n",
    "    outputs_bw = tf.transpose(outputs_bw, [1, 0, 2])\n",
    "    last_output_bw = tf.gather(outputs_bw, int(outputs_bw.get_shape()[0]) - 1)\n",
    "    \n",
    "    # Concat outputs\n",
    "    outputs_concat = tf.concat([last_output_fw, last_output_bw], 1)\n",
    "\n",
    "    # Fully connected\n",
    "    fc1 = tf.layers.dense(inputs=outputs_concat, units=32)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=fc1, units=2, activation=None)\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(X)\n",
    "        print(Y)\n",
    "        print(embedding)\n",
    "        print(outputs_concat)\n",
    "        print(fc1)\n",
    "        print(logits)\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inputs:0\", shape=(?, 7), dtype=int32)\n",
      "Tensor(\"output:0\", dtype=int32)\n",
      "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 7, 512), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "tensor_X = tf.placeholder(tf.int32, (None, X_train_mapped.shape[1]), 'inputs')\n",
    "tensor_Y = tf.placeholder(tf.int32, (None), 'output')\n",
    "\n",
    "#input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "#output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "# Create graph for the network\n",
    "logits = create_network(tensor_X, tensor_Y, vocab, embedding_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tensor_Y)\n",
    "loss = tf.reduce_mean(ce)\n",
    "\n",
    "# Using Adam (Adaptive learning rate + momentum) for the update of the weights of the network\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=1))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, tensor_Y))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations per epoch: 1105\n",
      "Loss: 0.6624677, Accuracy: 0.65625\n",
      "VALIDATION loss: 0.7041237, accuracy: 0.4990376\n",
      "Loss: 0.58844924, Accuracy: 0.75\n",
      "VALIDATION loss: 0.63115823, accuracy: 0.68761325\n",
      "Loss: 0.57296, Accuracy: 0.734375\n",
      "VALIDATION loss: 0.59675926, accuracy: 0.7189198\n",
      "Loss: 0.63503325, Accuracy: 0.65625\n",
      "VALIDATION loss: 0.5885399, accuracy: 0.720505\n",
      "Loss: 0.61275923, Accuracy: 0.6875\n",
      "VALIDATION loss: 0.585065, accuracy: 0.7210145\n",
      "Loss: 0.55083835, Accuracy: 0.75\n",
      "VALIDATION loss: 0.58160025, accuracy: 0.7211843\n",
      "Loss: 0.509092, Accuracy: 0.796875\n",
      "VALIDATION loss: 0.5783371, accuracy: 0.72129756\n",
      "Loss: 0.5601565, Accuracy: 0.78125\n",
      "VALIDATION loss: 0.5746887, accuracy: 0.7219769\n",
      "Loss: 0.52747357, Accuracy: 0.78125\n",
      "VALIDATION loss: 0.57209873, accuracy: 0.7225996\n",
      "Loss: 0.52562404, Accuracy: 0.75\n",
      "VALIDATION loss: 0.56810755, accuracy: 0.72356206\n",
      "Loss: 0.583038, Accuracy: 0.71875\n",
      "VALIDATION loss: 0.56391156, accuracy: 0.7244679\n",
      "Loss: 0.5565937, Accuracy: 0.703125\n",
      "VALIDATION loss: 0.56089854, accuracy: 0.7263927\n",
      "Training epoch: 1, AVG loss: 0.59059024, AVG accuracy: 0.70891243\n",
      "\n",
      "Loss: 0.48442745, Accuracy: 0.765625\n",
      "VALIDATION loss: 0.5605804, accuracy: 0.7265059\n",
      "Loss: 0.49492937, Accuracy: 0.78125\n",
      "VALIDATION loss: 0.55663145, accuracy: 0.72786456\n",
      "Loss: 0.53022265, Accuracy: 0.765625\n",
      "VALIDATION loss: 0.55238116, accuracy: 0.7303555\n",
      "Loss: 0.44856897, Accuracy: 0.8125\n",
      "VALIDATION loss: 0.54893947, accuracy: 0.7305254\n",
      "Loss: 0.55013275, Accuracy: 0.734375\n",
      "VALIDATION loss: 0.5451511, accuracy: 0.73448825\n",
      "Loss: 0.542997, Accuracy: 0.734375\n",
      "VALIDATION loss: 0.5400799, accuracy: 0.73692256\n",
      "Loss: 0.4915017, Accuracy: 0.8125\n",
      "VALIDATION loss: 0.53678864, accuracy: 0.7375453\n",
      "Loss: 0.5408328, Accuracy: 0.703125\n",
      "VALIDATION loss: 0.53235584, accuracy: 0.7376019\n",
      "Loss: 0.6107379, Accuracy: 0.65625\n",
      "VALIDATION loss: 0.5286154, accuracy: 0.7443954\n",
      "Loss: 0.5369861, Accuracy: 0.703125\n",
      "VALIDATION loss: 0.52426356, accuracy: 0.7493206\n",
      "Loss: 0.51494634, Accuracy: 0.78125\n",
      "VALIDATION loss: 0.51738626, accuracy: 0.7508492\n",
      "Loss: 0.48698044, Accuracy: 0.734375\n",
      "Training epoch: 2, AVG loss: 0.51579344, AVG accuracy: 0.74791735\n",
      "\n",
      "Loss: 0.39568645, Accuracy: 0.828125\n",
      "VALIDATION loss: 0.51402855, accuracy: 0.7509058\n",
      "Loss: 0.43049848, Accuracy: 0.859375\n",
      "VALIDATION loss: 0.51303047, accuracy: 0.75300044\n",
      "Loss: 0.38599324, Accuracy: 0.84375\n",
      "VALIDATION loss: 0.5108755, accuracy: 0.753793\n",
      "Loss: 0.45976222, Accuracy: 0.78125\n",
      "VALIDATION loss: 0.5071234, accuracy: 0.7581522\n",
      "Loss: 0.45578063, Accuracy: 0.765625\n",
      "VALIDATION loss: 0.5015525, accuracy: 0.76273775\n",
      "Loss: 0.31759727, Accuracy: 0.84375\n",
      "Loss: 0.39758864, Accuracy: 0.859375\n",
      "VALIDATION loss: 0.49560443, accuracy: 0.7642663\n",
      "Loss: 0.48218465, Accuracy: 0.734375\n",
      "VALIDATION loss: 0.48979375, accuracy: 0.769418\n",
      "Loss: 0.51238215, Accuracy: 0.78125\n",
      "VALIDATION loss: 0.4869805, accuracy: 0.7718524\n",
      "Loss: 0.38268572, Accuracy: 0.8125\n",
      "VALIDATION loss: 0.48190463, accuracy: 0.7727581\n",
      "Loss: 0.453302, Accuracy: 0.78125\n",
      "VALIDATION loss: 0.47828847, accuracy: 0.77734375\n",
      "Loss: 0.47385028, Accuracy: 0.765625\n",
      "Training epoch: 3, AVG loss: 0.4343048, AVG accuracy: 0.79898596\n",
      "\n",
      "Loss: 0.33918536, Accuracy: 0.859375\n",
      "Loss: 0.35347223, Accuracy: 0.875\n",
      "VALIDATION loss: 0.47182, accuracy: 0.78062725\n",
      "Loss: 0.36078143, Accuracy: 0.859375\n",
      "VALIDATION loss: 0.47120458, accuracy: 0.78102356\n",
      "Loss: 0.39812177, Accuracy: 0.796875\n",
      "VALIDATION loss: 0.4727835, accuracy: 0.78323144\n",
      "Loss: 0.3704285, Accuracy: 0.828125\n",
      "VALIDATION loss: 0.46854582, accuracy: 0.78691125\n",
      "Loss: 0.38849384, Accuracy: 0.859375\n",
      "VALIDATION loss: 0.4658346, accuracy: 0.78708106\n",
      "Loss: 0.39617273, Accuracy: 0.796875\n",
      "VALIDATION loss: 0.46139255, accuracy: 0.78776044\n",
      "Loss: 0.27717763, Accuracy: 0.921875\n",
      "VALIDATION loss: 0.46140003, accuracy: 0.78877944\n",
      "Loss: 0.38490334, Accuracy: 0.8125\n",
      "VALIDATION loss: 0.45541117, accuracy: 0.7931952\n",
      "Loss: 0.38313752, Accuracy: 0.8125\n",
      "VALIDATION loss: 0.454581, accuracy: 0.7956295\n",
      "Loss: 0.44228745, Accuracy: 0.78125\n",
      "VALIDATION loss: 0.45653966, accuracy: 0.79613906\n",
      "Loss: 0.47889173, Accuracy: 0.8125\n",
      "VALIDATION loss: 0.4484739, accuracy: 0.8010077\n",
      "Training epoch: 4, AVG loss: 0.357489, AVG accuracy: 0.8423521\n",
      "\n",
      "Loss: 0.16558918, Accuracy: 1.0\n",
      "VALIDATION loss: 0.44849992, accuracy: 0.80163044\n",
      "Loss: 0.35442716, Accuracy: 0.859375\n",
      "VALIDATION loss: 0.45145017, accuracy: 0.8018569\n",
      "Loss: 0.28441414, Accuracy: 0.90625\n",
      "Loss: 0.2785942, Accuracy: 0.859375\n",
      "Loss: 0.32124874, Accuracy: 0.890625\n",
      "VALIDATION loss: 0.45176983, accuracy: 0.8043478\n",
      "Loss: 0.29679084, Accuracy: 0.875\n",
      "VALIDATION loss: 0.4457017, accuracy: 0.80446106\n",
      "Loss: 0.2023175, Accuracy: 0.90625\n",
      "VALIDATION loss: 0.44651443, accuracy: 0.80717844\n",
      "Loss: 0.2948671, Accuracy: 0.890625\n",
      "VALIDATION loss: 0.4469449, accuracy: 0.8092165\n",
      "Loss: 0.3341012, Accuracy: 0.859375\n",
      "VALIDATION loss: 0.43700892, accuracy: 0.8098958\n",
      "Loss: 0.32930672, Accuracy: 0.90625\n",
      "VALIDATION loss: 0.43943456, accuracy: 0.81034875\n",
      "Loss: 0.32901335, Accuracy: 0.875\n",
      "VALIDATION loss: 0.43828306, accuracy: 0.81046194\n",
      "Loss: 0.1733942, Accuracy: 0.953125\n",
      "VALIDATION loss: 0.4337111, accuracy: 0.8141984\n",
      "Training epoch: 5, AVG loss: 0.29075718, AVG accuracy: 0.8771008\n",
      "\n",
      "Loss: 0.1787113, Accuracy: 0.953125\n",
      "VALIDATION loss: 0.43350205, accuracy: 0.8147079\n",
      "Loss: 0.17766635, Accuracy: 0.90625\n",
      "Loss: 0.2598798, Accuracy: 0.890625\n",
      "VALIDATION loss: 0.44284064, accuracy: 0.81538725\n",
      "Loss: 0.32386336, Accuracy: 0.859375\n",
      "VALIDATION loss: 0.44693622, accuracy: 0.8161798\n",
      "Loss: 0.2533474, Accuracy: 0.921875\n",
      "Loss: 0.276845, Accuracy: 0.859375\n",
      "Loss: 0.21164462, Accuracy: 0.90625\n",
      "VALIDATION loss: 0.44917575, accuracy: 0.8169724\n"
     ]
    }
   ],
   "source": [
    "# Training data variables\n",
    "iterations_training = max((len(X_train) // batch_size), 1)\n",
    "print(\"Training iterations per epoch: \" + str(iterations_training))\n",
    "training_overfit = False\n",
    "consecutive_validation_without_saving = 0\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "iterations_validation = max((len(X_final_val) // batch_size), 1)\n",
    "\n",
    "# Initializer for variables in the graph\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Perform each epoch, shuffle training dataset\n",
    "indices = list(range(len(X_train)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables in the graph\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle data (with random seed for debug) to not train the network always with the same order\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_mapped = X_train_mapped[indices]\n",
    "        y_train = y_train[indices]\n",
    "        \n",
    "        # Vector accumulating accuracy and loss during for one epoch\n",
    "        total_accuracies, total_losses = [], []\n",
    "\n",
    "        # Iterate over mini-batches\n",
    "        for j in range(iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size \n",
    "            \n",
    "            # If last batch, take also elements that are less than batch_size\n",
    "            if j == (iterations_training - 1):\n",
    "                end_index += (batch_size - 1)\n",
    "\n",
    "            _, avg_accuracy, avg_loss = sess.run([optimizer, accuracy, loss], feed_dict={\n",
    "                                            tensor_X: X_train_mapped[start_index:end_index],\n",
    "                                            tensor_Y: y_train[start_index:end_index] })\n",
    "            \n",
    "            # Add values for this mini-batch iterations\n",
    "            total_losses.append(avg_loss) \n",
    "            total_accuracies.append(avg_accuracy)\n",
    "\n",
    "            \n",
    "            # Print loss and accuracy\n",
    "            if (j) % 100 == 0:\n",
    "                print(\"Loss: \" + str(avg_loss) + \", Accuracy: \" + str(avg_accuracy))\n",
    "                \n",
    "            # Statistics on validation set\n",
    "            if (j) % 100 == 0:\n",
    "                \n",
    "                # Accumulate validation statistics\n",
    "                val_accuracies, val_losses = [], []\n",
    "\n",
    "                # Iterate over mini-batches\n",
    "                for k in range(iterations_validation):\n",
    "                    start_index = k * batch_size\n",
    "                    end_index = (k + 1) * batch_size \n",
    "                    \n",
    "                    if j == (iterations_validation - 1):\n",
    "                        end_index += (batch_size - 1)\n",
    "                        \n",
    "                    avg_accuracy, avg_loss = sess.run([accuracy, loss], feed_dict={\n",
    "                                            tensor_X: X_final_val[start_index:end_index],\n",
    "                                            tensor_Y: y_final_val[start_index:end_index] })\n",
    "                    \n",
    "                    # Statistics over the mini-batch\n",
    "                    val_losses.append(avg_loss) \n",
    "                    val_accuracies.append(avg_accuracy)\n",
    "                    \n",
    "                # Average validation accuracy over batches\n",
    "                final_val_accuracy = np.mean(val_accuracies)\n",
    "                \n",
    "                # Save model if validation accuracy better\n",
    "                if final_val_accuracy > max_val_acc:\n",
    "                    consecutive_validation_without_saving = 0\n",
    "                    max_val_acc = final_val_accuracy\n",
    "                    print(\"VALIDATION loss: \" + str(np.mean(val_losses)) + \", accuracy: \" + str(final_val_accuracy))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")\n",
    "                else:\n",
    "                    # Count every time check validation accuracy\n",
    "                    consecutive_validation_without_saving += 1\n",
    "                \n",
    "                # If checked validation time many consecutive times without having improvement in accuracy\n",
    "                if consecutive_validation_without_saving >= 20:\n",
    "                    training_overfit = True\n",
    "                    break\n",
    "                    \n",
    "        if training_overfit:\n",
    "            print(\"Early stopping training because it starts overfitting\")\n",
    "            break\n",
    "            \n",
    "        # Epoch statistics\n",
    "        print(\"Training epoch: \" + str(i+1) + \", AVG loss: \" + str(np.mean(np.array(total_losses))) + \n",
    "              \", AVG accuracy: \" + str(np.mean(np.array(total_accuracies))) + \"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
