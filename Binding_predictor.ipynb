{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import dictionary\n",
    "\n",
    "# Update python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 1\n",
    "sliding_window_size = 9\n",
    "embedding_size = 512\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">P0A8Q6\n",
      "MGKTNDWLDFDQLAEEKVRDALKPPSMYKVILVNDDYTPMEFVIDVLQKFFSYDVERATQLMLAVHYQGKAICGVFTAEVAETKVAMVNKYARENEHPLLCTLEKA\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for prediction\n",
    "protein_names, sequences, labels = [], [], []\n",
    "\n",
    "\n",
    "'''\n",
    "    Labels:\n",
    "        \"+\" stands for \"binding protein\" => 1\n",
    "        \"-\" stands for \"non-binding\" => 0\n",
    "'''\n",
    "def convert_label(label_string):\n",
    " \n",
    "    if label_string == \"+\":\n",
    "        return 1\n",
    "    elif label_string == \"-\":\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"Should not enter here\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "# Open file containing dataset    \n",
    "with open('./ppi_data.fasta') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        \n",
    "        if i % 3 == 0:\n",
    "            protein_names.append(lines[i])\n",
    "        elif i % 3 == 1:\n",
    "            sequences.append(lines[i])\n",
    "        elif i % 3 == 2:\n",
    "            labels.append([convert_label(letter) for letter in lines[i]])\n",
    "            \n",
    "protein_names = np.array(protein_names)\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "assert(protein_names.shape[0] == sequences.shape[0] == labels.shape[0])\n",
    "\n",
    "print(protein_names[0])\n",
    "print(sequences[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences = sequences[:15]\n",
    "#labels = labels[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 432\n",
      "Validation samples: 108\n",
      "\n",
      "MTDLFSSPDHTLDALGLRCPEPVMMVRKTVRNMQPGETLLIIADDPATTRDIPGFCTFMEHELVAKETDGLPYRYLIRKGG\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Split percentage of training and validation\n",
    "split_percentage = 0.8\n",
    "\n",
    "# Count how many samples into training dataset\n",
    "total_dataset = len(sequences)\n",
    "train_dataset = int(total_dataset * split_percentage)\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "indices = list(range(total_dataset))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Train dataset\n",
    "sequences_train = sequences[indices[:train_dataset]]\n",
    "labels_train = labels[indices[:train_dataset]]\n",
    "\n",
    "# Validation dataset\n",
    "sequences_val = sequences[indices[train_dataset:]]\n",
    "labels_val = labels[indices[train_dataset:]]\n",
    "\n",
    "# Shapes\n",
    "print(\"Training samples: \" + str(sequences_train.shape[0]))\n",
    "print(\"Validation samples: \" + str(sequences_val.shape[0]) + \"\\n\")\n",
    "\n",
    "# Reset seed for randomness\n",
    "np.random.seed()\n",
    "\n",
    "# Sample\n",
    "print(sequences_train[0])\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70799, 9)\n",
      "(17662, 9)\n",
      "['M' 'T' 'D' 'L' 'F' 'S' 'S' 'P' 'D']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def create_input(sequences, labels, sliding_window_size, n_grams):\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Iterates over all proteins in dataset\n",
    "    for i in range(len(sequences)):\n",
    "\n",
    "        # Loop over sequence\n",
    "        for j in range(0,len(sequences[i]) - sliding_window_size + 1):\n",
    "            sub_sequence = sequences[i][j:j+sliding_window_size]\n",
    "\n",
    "            tmp = []\n",
    "            for k in range(0, sliding_window_size - n_grams + 1):\n",
    "                tmp.append(sub_sequence[k:k+n_grams])\n",
    "\n",
    "            X.append(tmp)\n",
    "            Y.append(labels[i][j+(sliding_window_size//2)]) \n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "    \n",
    "X_train_tmp, Y_train_tmp = create_input(sequences_train, labels_train, sliding_window_size, n_grams)\n",
    "X_val, Y_val = create_input(sequences_val, labels_val, sliding_window_size, n_grams)\n",
    "\n",
    "print(X_train_tmp.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(X_train_tmp[0])\n",
    "print(Y_train_tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19705,)\n",
      "(51094,)\n",
      "(102188, 9)\n",
      "(102188,)\n"
     ]
    }
   ],
   "source": [
    "# Find indices with positive and negative labels\n",
    "X_train_pos = np.array([i for i in range(len(X_train_tmp)) if Y_train_tmp[i] == 1])\n",
    "X_train_neg = np.array([i for i in range(len(X_train_tmp)) if Y_train_tmp[i] == 0])\n",
    "\n",
    "# Print how many positive and negative labels => I want same number of labels for each class during training\n",
    "print(X_train_pos.shape)\n",
    "print(X_train_neg.shape)\n",
    "\n",
    "# Get indices from X_train_pos\n",
    "np.random.seed(42)\n",
    "X_train_pos_indices = np.random.choice(len(X_train_pos), len(X_train_neg), replace=True)\n",
    "X_train_selected = X_train_pos[X_train_pos_indices]\n",
    "\n",
    "# Final X_train data\n",
    "X_train = np.concatenate((X_train_tmp[X_train_selected], X_train_tmp[X_train_neg]), axis=0)\n",
    "Y_train = np.concatenate((Y_train_tmp[X_train_selected], Y_train_tmp[X_train_neg]), axis=0)\n",
    "\n",
    "# Check that labels 1 and 0 are equal\n",
    "assert(len(np.array([i for i in range(len(X_train)) if Y_train[i] == 1])) == \n",
    "    len(np.array([i for i in range(len(X_train)) if Y_train[i] == 0])))\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_indices = list(range(len(X_train)))\n",
    "np.random.shuffle(shuffle_indices)\n",
    "X_train = X_train[shuffle_indices]\n",
    "Y_train = Y_train[shuffle_indices]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17662, 9)\n",
      "(17662,)\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary of n-grams\n",
    "vocab = dictionary.LanguageDictionary(X_train)\n",
    "\n",
    "# Map grams to indices for the embedding matrix and remove samples where unknown words\n",
    "X_train_mapped = np.array([vocab.text_to_indices(tmp) for tmp in X_train])\n",
    "\n",
    "# Prepare validation data\n",
    "X_val_mapped = []\n",
    "Y_val_mapped = []\n",
    "for i in range(len(X_val)):\n",
    "    \n",
    "    tmp = vocab.text_to_indices(X_val[i])\n",
    "    if not None in tmp:\n",
    "        X_val_mapped.append(tmp)\n",
    "        Y_val_mapped.append(Y_val[i])\n",
    "        \n",
    "X_val_mapped = np.array(X_val_mapped)\n",
    "Y_val_mapped = np.array(Y_val_mapped)\n",
    "\n",
    "print(X_val_mapped.shape)\n",
    "print(Y_val_mapped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def max_length_sentence(dataset):\\n    return max([len(line) for line in dataset])\\n\\n\\ndef pad_sentence(tokenized_sentence, max_length_sentence, padding_value=0, pad_before=False):\\n    \\n    pad_length = max_length_sentence - len(tokenized_sentence)\\n    sentence = list(tokenized_sentence)\\n    \\n    if pad_length > 0:\\n        if pad_before:\\n            return np.pad(tokenized_sentence, (pad_length, 0), mode='constant', constant_values=int(padding_value))\\n        else:\\n            return np.pad(tokenized_sentence, (0, pad_length), mode='constant', constant_values=int(padding_value))\\n    else: # Cut sequence if longer than max_length_sentence\\n        return sentence[:max_length_sentence]\\n\\n    \\nmax_length = max_length_sentence(sequences_train)\\n\\n\\nX_train = np.array([pad_sentence(tmp, max_length) for tmp in X_train_mapped])\\nY_train = np.array([pad_sentence(tmp, max_length, padding_value=1) for tmp in labels_train])\\nprint(X_train.shape)\\nprint(Y_train.shape)\\n\\nX_val = np.array([pad_sentence(tmp, max_length) for tmp in X_val_mapped])\\nY_val = np.array([pad_sentence(tmp, max_length, padding_value=1) for tmp in labels_val])\\nprint(X_val.shape)\\nprint(Y_val.shape)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def max_length_sentence(dataset):\n",
    "    return max([len(line) for line in dataset])\n",
    "\n",
    "\n",
    "def pad_sentence(tokenized_sentence, max_length_sentence, padding_value=0, pad_before=False):\n",
    "    \n",
    "    pad_length = max_length_sentence - len(tokenized_sentence)\n",
    "    sentence = list(tokenized_sentence)\n",
    "    \n",
    "    if pad_length > 0:\n",
    "        if pad_before:\n",
    "            return np.pad(tokenized_sentence, (pad_length, 0), mode='constant', constant_values=int(padding_value))\n",
    "        else:\n",
    "            return np.pad(tokenized_sentence, (0, pad_length), mode='constant', constant_values=int(padding_value))\n",
    "    else: # Cut sequence if longer than max_length_sentence\n",
    "        return sentence[:max_length_sentence]\n",
    "\n",
    "    \n",
    "max_length = max_length_sentence(sequences_train)\n",
    "\n",
    "\n",
    "X_train = np.array([pad_sentence(tmp, max_length) for tmp in X_train_mapped])\n",
    "Y_train = np.array([pad_sentence(tmp, max_length, padding_value=1) for tmp in labels_train])\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_val = np.array([pad_sentence(tmp, max_length) for tmp in X_val_mapped])\n",
    "Y_val = np.array([pad_sentence(tmp, max_length, padding_value=1) for tmp in labels_val])\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape, name=None):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "\n",
    "def new_biases(length, name=None):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[length]), name=name)\n",
    "\n",
    "\n",
    "def embedding_layer(input_x, vocabulary_size, embedding_size):\n",
    "    init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "    embeddings = tf.Variable(init_embeds)\n",
    "    layer = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "def create_network(X, Y, vocabulary, embedding_size, verbose):\n",
    "    \n",
    "    embedding = embedding_layer(X, len(vocabulary.index_to_word), embedding_size)\n",
    "    \n",
    "    \n",
    "    lstm_fw_cell = tf.contrib.rnn.LSTMCell(128, forget_bias=1.0)\n",
    "    lstm_bw_cell = tf.contrib.rnn.LSTMCell(128, forget_bias=1.0)\n",
    "    (outputs_fw, outputs_bw), last_states = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedding, dtype=tf.float32)\n",
    "\n",
    "    # Get last output of LSTM\n",
    "    outputs_fw = tf.transpose(outputs_fw, [1, 0, 2])\n",
    "    last_output_fw = tf.gather(outputs_fw, int(outputs_fw.get_shape()[0]) - 1)\n",
    "\n",
    "    outputs_bw = tf.transpose(outputs_bw, [1, 0, 2])\n",
    "    last_output_bw = tf.gather(outputs_bw, int(outputs_bw.get_shape()[0]) - 1)\n",
    "    \n",
    "    # Concat outputs\n",
    "    outputs_concat = tf.concat([last_output_fw, last_output_bw], 1)\n",
    "\n",
    "    # Fully connected\n",
    "    fc1 = tf.layers.dense(inputs=outputs_concat, units=64)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=fc1, units=2, activation=None)\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(X)\n",
    "        print(Y)\n",
    "        print(embedding)\n",
    "        print(outputs_concat)\n",
    "        print(fc1)\n",
    "        print(logits)\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inputs:0\", shape=(?, 9), dtype=int32)\n",
      "Tensor(\"output:0\", dtype=int32)\n",
      "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 9, 512), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "tensor_X = tf.placeholder(tf.int32, (None, X_train_mapped.shape[1]), 'inputs')\n",
    "tensor_Y = tf.placeholder(tf.int32, (None), 'output')\n",
    "\n",
    "#input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "#output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "# Create graph for the network\n",
    "logits = create_network(tensor_X, tensor_Y, vocab, embedding_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tensor_Y)\n",
    "loss = tf.reduce_mean(ce)\n",
    "\n",
    "# Using Adam (Adaptive learning rate + momentum) for the update of the weights of the network\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=1))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, tensor_Y))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations per epoch: 3193\n",
      "Loss: 0.72264683, Accuracy: 0.53125\n",
      "VALIDATION loss: 0.65414816, accuracy: 0.64025635\n",
      "Loss: 0.6993661, Accuracy: 0.46875\n",
      "Loss: 0.73302424, Accuracy: 0.46875\n",
      "Loss: 0.67934746, Accuracy: 0.59375\n",
      "Loss: 0.67359316, Accuracy: 0.53125\n",
      "Loss: 0.6587292, Accuracy: 0.5625\n",
      "Loss: 0.6704196, Accuracy: 0.53125\n",
      "Loss: 0.6672443, Accuracy: 0.625\n",
      "Loss: 0.697526, Accuracy: 0.5\n",
      "Loss: 0.72128445, Accuracy: 0.40625\n",
      "Loss: 0.70684975, Accuracy: 0.59375\n",
      "Loss: 0.79136455, Accuracy: 0.34375\n",
      "Loss: 0.74661684, Accuracy: 0.46875\n",
      "Loss: 0.69505733, Accuracy: 0.53125\n",
      "Loss: 0.6881025, Accuracy: 0.59375\n",
      "Loss: 0.67779255, Accuracy: 0.59375\n",
      "Loss: 0.63059306, Accuracy: 0.5625\n",
      "Loss: 0.66302514, Accuracy: 0.5625\n",
      "Loss: 0.6652262, Accuracy: 0.65625\n",
      "Loss: 0.65966666, Accuracy: 0.625\n",
      "Loss: 0.6354507, Accuracy: 0.75\n",
      "Loss: 0.7217624, Accuracy: 0.5\n",
      "Loss: 0.6742359, Accuracy: 0.5625\n",
      "Loss: 0.6495132, Accuracy: 0.59375\n",
      "Loss: 0.7414073, Accuracy: 0.4375\n",
      "Loss: 0.6919689, Accuracy: 0.5625\n",
      "Loss: 0.7213805, Accuracy: 0.46875\n",
      "Loss: 0.65076625, Accuracy: 0.625\n",
      "Loss: 0.6278454, Accuracy: 0.71875\n",
      "Loss: 0.63261545, Accuracy: 0.625\n",
      "Loss: 0.6597241, Accuracy: 0.5625\n",
      "Loss: 0.64306724, Accuracy: 0.65625\n",
      "Training epoch: 1, AVG loss: 0.68026066, AVG accuracy: 0.562645\n",
      "\n",
      "Loss: 0.71775734, Accuracy: 0.53125\n",
      "Loss: 0.666844, Accuracy: 0.59375\n",
      "Loss: 0.70197105, Accuracy: 0.40625\n",
      "Loss: 0.7054255, Accuracy: 0.53125\n",
      "Loss: 0.64324903, Accuracy: 0.59375\n",
      "Loss: 0.6459767, Accuracy: 0.59375\n",
      "Loss: 0.652002, Accuracy: 0.65625\n",
      "Loss: 0.687919, Accuracy: 0.625\n",
      "Loss: 0.6904675, Accuracy: 0.5625\n",
      "Loss: 0.61312765, Accuracy: 0.6875\n",
      "Loss: 0.65389395, Accuracy: 0.65625\n",
      "Loss: 0.60570365, Accuracy: 0.71875\n",
      "Loss: 0.6729355, Accuracy: 0.46875\n",
      "Loss: 0.691054, Accuracy: 0.5625\n",
      "Loss: 0.6642102, Accuracy: 0.59375\n",
      "Loss: 0.65119267, Accuracy: 0.625\n",
      "Loss: 0.59831965, Accuracy: 0.71875\n",
      "Loss: 0.55202687, Accuracy: 0.8125\n",
      "Loss: 0.68246007, Accuracy: 0.53125\n",
      "Loss: 0.6459039, Accuracy: 0.71875\n",
      "Loss: 0.54879427, Accuracy: 0.78125\n",
      "Loss: 0.6366944, Accuracy: 0.65625\n",
      "Loss: 0.6338444, Accuracy: 0.65625\n",
      "Loss: 0.5936916, Accuracy: 0.71875\n",
      "Loss: 0.59797204, Accuracy: 0.6875\n",
      "Loss: 0.7044982, Accuracy: 0.46875\n",
      "Loss: 0.6790246, Accuracy: 0.59375\n",
      "Loss: 0.62448573, Accuracy: 0.65625\n",
      "Loss: 0.73031044, Accuracy: 0.5625\n",
      "Loss: 0.55100566, Accuracy: 0.75\n",
      "Loss: 0.6335891, Accuracy: 0.6875\n",
      "Loss: 0.6921888, Accuracy: 0.5625\n",
      "Training epoch: 2, AVG loss: 0.6609443, AVG accuracy: 0.59469134\n",
      "\n",
      "Loss: 0.656019, Accuracy: 0.5\n",
      "Loss: 0.58415854, Accuracy: 0.75\n",
      "Loss: 0.6690328, Accuracy: 0.5625\n",
      "Loss: 0.6650624, Accuracy: 0.625\n",
      "Loss: 0.65775204, Accuracy: 0.59375\n",
      "Loss: 0.63816, Accuracy: 0.625\n",
      "Loss: 0.6244835, Accuracy: 0.59375\n",
      "Loss: 0.6263472, Accuracy: 0.59375\n",
      "Loss: 0.70379704, Accuracy: 0.5625\n",
      "Loss: 0.64076424, Accuracy: 0.6875\n",
      "Loss: 0.6085291, Accuracy: 0.75\n",
      "Loss: 0.6881287, Accuracy: 0.5\n",
      "Loss: 0.59648454, Accuracy: 0.65625\n",
      "Loss: 0.48801392, Accuracy: 0.8125\n",
      "Loss: 0.7308177, Accuracy: 0.5\n",
      "Loss: 0.6258751, Accuracy: 0.59375\n",
      "Loss: 0.64922005, Accuracy: 0.625\n",
      "Loss: 0.6299736, Accuracy: 0.625\n",
      "Loss: 0.60723925, Accuracy: 0.6875\n",
      "Loss: 0.6085499, Accuracy: 0.75\n",
      "Loss: 0.64152265, Accuracy: 0.5625\n",
      "Loss: 0.6707417, Accuracy: 0.59375\n",
      "Loss: 0.60554385, Accuracy: 0.75\n",
      "Loss: 0.60500324, Accuracy: 0.6875\n",
      "Loss: 0.64543617, Accuracy: 0.6875\n",
      "Loss: 0.63681763, Accuracy: 0.65625\n",
      "Loss: 0.5982024, Accuracy: 0.65625\n",
      "Loss: 0.6361835, Accuracy: 0.59375\n",
      "Loss: 0.5718684, Accuracy: 0.71875\n",
      "Loss: 0.63538134, Accuracy: 0.59375\n",
      "Loss: 0.64176387, Accuracy: 0.59375\n",
      "Loss: 0.67463934, Accuracy: 0.53125\n",
      "Training epoch: 3, AVG loss: 0.63949186, AVG accuracy: 0.6273053\n",
      "\n",
      "Loss: 0.5814563, Accuracy: 0.6875\n",
      "Loss: 0.6724538, Accuracy: 0.53125\n",
      "Loss: 0.5996925, Accuracy: 0.6875\n",
      "Loss: 0.58516204, Accuracy: 0.6875\n",
      "Loss: 0.6032319, Accuracy: 0.625\n",
      "Loss: 0.632061, Accuracy: 0.6875\n",
      "Loss: 0.6133587, Accuracy: 0.6875\n",
      "Loss: 0.6816086, Accuracy: 0.65625\n",
      "Loss: 0.5939326, Accuracy: 0.6875\n",
      "Loss: 0.6141803, Accuracy: 0.5625\n",
      "Loss: 0.69946754, Accuracy: 0.5625\n",
      "Loss: 0.5930973, Accuracy: 0.71875\n",
      "Loss: 0.49238247, Accuracy: 0.8125\n",
      "Loss: 0.63551426, Accuracy: 0.6875\n",
      "Loss: 0.5884045, Accuracy: 0.8125\n",
      "Loss: 0.66255647, Accuracy: 0.625\n",
      "Loss: 0.7089752, Accuracy: 0.625\n",
      "Loss: 0.55042195, Accuracy: 0.75\n",
      "Loss: 0.56756425, Accuracy: 0.78125\n",
      "Loss: 0.5457519, Accuracy: 0.65625\n",
      "Loss: 0.61534333, Accuracy: 0.6875\n",
      "Loss: 0.64943796, Accuracy: 0.625\n",
      "Loss: 0.6328895, Accuracy: 0.5625\n",
      "Loss: 0.50421596, Accuracy: 0.75\n",
      "Loss: 0.5085455, Accuracy: 0.71875\n",
      "Loss: 0.6029873, Accuracy: 0.625\n",
      "Loss: 0.71830344, Accuracy: 0.5625\n",
      "Loss: 0.69611794, Accuracy: 0.65625\n",
      "Loss: 0.6572778, Accuracy: 0.625\n",
      "Loss: 0.59030503, Accuracy: 0.625\n",
      "Loss: 0.5243379, Accuracy: 0.6875\n",
      "Loss: 0.53339005, Accuracy: 0.6875\n",
      "Training epoch: 4, AVG loss: 0.6093588, AVG accuracy: 0.66120046\n",
      "\n",
      "Loss: 0.6908635, Accuracy: 0.59375\n",
      "Loss: 0.592985, Accuracy: 0.75\n",
      "Loss: 0.5419232, Accuracy: 0.71875\n",
      "Loss: 0.62793446, Accuracy: 0.65625\n",
      "Loss: 0.51339364, Accuracy: 0.71875\n",
      "Loss: 0.59057665, Accuracy: 0.65625\n",
      "Loss: 0.5028908, Accuracy: 0.71875\n",
      "Loss: 0.5614122, Accuracy: 0.75\n",
      "Loss: 0.55788505, Accuracy: 0.6875\n",
      "Loss: 0.6825579, Accuracy: 0.53125\n",
      "Loss: 0.639878, Accuracy: 0.6875\n",
      "Loss: 0.7663752, Accuracy: 0.53125\n",
      "Loss: 0.70279723, Accuracy: 0.625\n",
      "Loss: 0.40693185, Accuracy: 0.90625\n",
      "Loss: 0.5300777, Accuracy: 0.75\n",
      "Loss: 0.5916382, Accuracy: 0.65625\n",
      "Loss: 0.60315883, Accuracy: 0.59375\n",
      "Loss: 0.5579604, Accuracy: 0.75\n",
      "Loss: 0.6133871, Accuracy: 0.625\n",
      "Loss: 0.7460424, Accuracy: 0.5\n",
      "Loss: 0.5455719, Accuracy: 0.65625\n",
      "Loss: 0.58083975, Accuracy: 0.65625\n",
      "Loss: 0.56361985, Accuracy: 0.65625\n",
      "Loss: 0.62649363, Accuracy: 0.6875\n",
      "Loss: 0.5331539, Accuracy: 0.6875\n",
      "Loss: 0.4872425, Accuracy: 0.75\n",
      "Loss: 0.6617264, Accuracy: 0.625\n",
      "Loss: 0.6091938, Accuracy: 0.65625\n",
      "Loss: 0.5525274, Accuracy: 0.78125\n",
      "Loss: 0.60939765, Accuracy: 0.625\n",
      "Loss: 0.5002248, Accuracy: 0.75\n",
      "Loss: 0.6002207, Accuracy: 0.5625\n",
      "Training epoch: 5, AVG loss: 0.5716843, AVG accuracy: 0.69748545\n",
      "\n",
      "Loss: 0.5782292, Accuracy: 0.625\n",
      "Loss: 0.49912715, Accuracy: 0.75\n",
      "Loss: 0.46241227, Accuracy: 0.8125\n",
      "Loss: 0.6023457, Accuracy: 0.5625\n",
      "Loss: 0.6147865, Accuracy: 0.6875\n",
      "Loss: 0.49180827, Accuracy: 0.78125\n",
      "Loss: 0.54753375, Accuracy: 0.75\n",
      "Loss: 0.5638642, Accuracy: 0.625\n",
      "Loss: 0.5027343, Accuracy: 0.6875\n",
      "Loss: 0.54606605, Accuracy: 0.75\n",
      "Loss: 0.5106499, Accuracy: 0.6875\n",
      "Loss: 0.51702476, Accuracy: 0.75\n",
      "Loss: 0.50972676, Accuracy: 0.71875\n",
      "Loss: 0.51833653, Accuracy: 0.75\n",
      "Loss: 0.5025809, Accuracy: 0.71875\n",
      "Loss: 0.4837864, Accuracy: 0.75\n",
      "Loss: 0.48304626, Accuracy: 0.8125\n",
      "Loss: 0.65915567, Accuracy: 0.59375\n",
      "Loss: 0.523332, Accuracy: 0.71875\n",
      "Loss: 0.56988615, Accuracy: 0.6875\n",
      "Loss: 0.48067886, Accuracy: 0.75\n",
      "Loss: 0.51008016, Accuracy: 0.78125\n",
      "Loss: 0.58880156, Accuracy: 0.65625\n",
      "Loss: 0.69707084, Accuracy: 0.65625\n",
      "Loss: 0.5806829, Accuracy: 0.625\n",
      "Loss: 0.6948403, Accuracy: 0.59375\n",
      "Loss: 0.6012076, Accuracy: 0.71875\n",
      "Loss: 0.63024855, Accuracy: 0.625\n",
      "Loss: 0.5369815, Accuracy: 0.71875\n",
      "Loss: 0.41952252, Accuracy: 0.78125\n",
      "Loss: 0.55593187, Accuracy: 0.71875\n",
      "Loss: 0.7315861, Accuracy: 0.59375\n",
      "Training epoch: 6, AVG loss: 0.5328625, AVG accuracy: 0.7307516\n",
      "\n",
      "Loss: 0.6013937, Accuracy: 0.6875\n",
      "Loss: 0.44643164, Accuracy: 0.75\n",
      "Loss: 0.69946307, Accuracy: 0.65625\n",
      "Loss: 0.46468163, Accuracy: 0.8125\n",
      "Loss: 0.30441737, Accuracy: 0.90625\n",
      "Loss: 0.50656676, Accuracy: 0.8125\n",
      "Loss: 0.37793297, Accuracy: 0.875\n",
      "Loss: 0.5795357, Accuracy: 0.6875\n",
      "Loss: 0.6870173, Accuracy: 0.6875\n",
      "Loss: 0.48523, Accuracy: 0.84375\n",
      "Loss: 0.49684164, Accuracy: 0.75\n",
      "Loss: 0.4948936, Accuracy: 0.78125\n",
      "Loss: 0.4213922, Accuracy: 0.8125\n",
      "Loss: 0.43438387, Accuracy: 0.84375\n",
      "Loss: 0.44331187, Accuracy: 0.75\n",
      "Loss: 0.4238125, Accuracy: 0.84375\n",
      "Loss: 0.63640463, Accuracy: 0.6875\n",
      "Loss: 0.41227382, Accuracy: 0.875\n",
      "Loss: 0.407431, Accuracy: 0.71875\n",
      "Loss: 0.5215279, Accuracy: 0.78125\n",
      "Loss: 0.46136156, Accuracy: 0.78125\n",
      "Loss: 0.7416483, Accuracy: 0.625\n",
      "Loss: 0.5450492, Accuracy: 0.65625\n",
      "Loss: 0.4380415, Accuracy: 0.8125\n",
      "Loss: 0.5307627, Accuracy: 0.78125\n",
      "Loss: 0.53464335, Accuracy: 0.75\n",
      "Loss: 0.32738483, Accuracy: 0.875\n",
      "Loss: 0.57361406, Accuracy: 0.75\n",
      "Loss: 0.5427407, Accuracy: 0.78125\n",
      "Loss: 0.40496093, Accuracy: 0.8125\n",
      "Loss: 0.5093384, Accuracy: 0.8125\n",
      "Loss: 0.59082246, Accuracy: 0.65625\n",
      "Training epoch: 7, AVG loss: 0.49514365, AVG accuracy: 0.75937504\n",
      "\n",
      "Loss: 0.5375457, Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3123756, Accuracy: 0.875\n",
      "Loss: 0.4226761, Accuracy: 0.78125\n",
      "Loss: 0.57688785, Accuracy: 0.65625\n",
      "Loss: 0.60094714, Accuracy: 0.75\n",
      "Loss: 0.56137985, Accuracy: 0.71875\n",
      "Loss: 0.4705498, Accuracy: 0.8125\n",
      "Loss: 0.60218656, Accuracy: 0.625\n",
      "Loss: 0.5687554, Accuracy: 0.625\n",
      "Loss: 0.445229, Accuracy: 0.75\n",
      "Loss: 0.4452623, Accuracy: 0.71875\n",
      "Loss: 0.39030454, Accuracy: 0.8125\n",
      "Loss: 0.44109452, Accuracy: 0.78125\n",
      "Loss: 0.31831124, Accuracy: 0.9375\n",
      "Loss: 0.4212801, Accuracy: 0.84375\n",
      "Loss: 0.39368972, Accuracy: 0.8125\n",
      "Loss: 0.31898314, Accuracy: 0.96875\n",
      "Loss: 0.31178465, Accuracy: 0.90625\n",
      "Loss: 0.5976896, Accuracy: 0.65625\n",
      "Loss: 0.49866766, Accuracy: 0.78125\n",
      "Loss: 0.499685, Accuracy: 0.78125\n",
      "Loss: 0.46898642, Accuracy: 0.6875\n",
      "Loss: 0.39917496, Accuracy: 0.8125\n",
      "Loss: 0.5092752, Accuracy: 0.78125\n",
      "Loss: 0.4485217, Accuracy: 0.71875\n",
      "Loss: 0.31891423, Accuracy: 0.84375\n",
      "Loss: 0.509223, Accuracy: 0.71875\n",
      "Loss: 0.4236986, Accuracy: 0.78125\n",
      "Loss: 0.47360513, Accuracy: 0.75\n",
      "Loss: 0.33595175, Accuracy: 0.875\n",
      "Loss: 0.41148496, Accuracy: 0.8125\n",
      "Loss: 0.52402776, Accuracy: 0.75\n",
      "Training epoch: 8, AVG loss: 0.46297172, AVG accuracy: 0.78028995\n",
      "\n",
      "Loss: 0.45705658, Accuracy: 0.8125\n",
      "Loss: 0.47909132, Accuracy: 0.78125\n",
      "Loss: 0.49485147, Accuracy: 0.71875\n",
      "Loss: 0.3770605, Accuracy: 0.875\n",
      "Loss: 0.48876745, Accuracy: 0.75\n",
      "Loss: 0.5010233, Accuracy: 0.71875\n",
      "Loss: 0.5374967, Accuracy: 0.75\n",
      "Loss: 0.46233958, Accuracy: 0.75\n",
      "Loss: 0.450909, Accuracy: 0.78125\n",
      "Loss: 0.36997706, Accuracy: 0.875\n",
      "Loss: 0.52240294, Accuracy: 0.6875\n",
      "Loss: 0.31332153, Accuracy: 0.875\n",
      "Loss: 0.37596387, Accuracy: 0.8125\n",
      "Loss: 0.533244, Accuracy: 0.75\n",
      "Loss: 0.3587611, Accuracy: 0.8125\n",
      "Loss: 0.44252887, Accuracy: 0.8125\n",
      "Loss: 0.52325237, Accuracy: 0.71875\n",
      "Loss: 0.46902776, Accuracy: 0.84375\n",
      "Loss: 0.5401712, Accuracy: 0.625\n",
      "Loss: 0.4292434, Accuracy: 0.75\n",
      "Loss: 0.6348501, Accuracy: 0.59375\n",
      "Loss: 0.40041664, Accuracy: 0.84375\n",
      "Loss: 0.33026114, Accuracy: 0.8125\n",
      "Loss: 0.4000101, Accuracy: 0.84375\n",
      "Loss: 0.5709564, Accuracy: 0.78125\n",
      "Loss: 0.51022446, Accuracy: 0.6875\n",
      "Loss: 0.45586514, Accuracy: 0.84375\n",
      "Loss: 0.5460207, Accuracy: 0.65625\n",
      "Loss: 0.5406898, Accuracy: 0.8125\n",
      "Loss: 0.34518597, Accuracy: 0.78125\n",
      "Loss: 0.45449758, Accuracy: 0.8125\n",
      "Loss: 0.45735347, Accuracy: 0.78125\n",
      "Training epoch: 9, AVG loss: 0.43281832, AVG accuracy: 0.80051357\n",
      "\n",
      "Loss: 0.34977126, Accuracy: 0.84375\n",
      "Loss: 0.3805697, Accuracy: 0.875\n",
      "Loss: 0.34806973, Accuracy: 0.84375\n",
      "Loss: 0.32177186, Accuracy: 0.875\n",
      "Loss: 0.5223892, Accuracy: 0.71875\n",
      "Loss: 0.40377423, Accuracy: 0.78125\n",
      "Loss: 0.4597479, Accuracy: 0.71875\n",
      "Loss: 0.2891558, Accuracy: 0.9375\n",
      "Loss: 0.45175755, Accuracy: 0.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-34709f3f7338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                     avg_accuracy, avg_loss = sess.run([accuracy, loss], feed_dict={\n\u001b[1;32m     71\u001b[0m                                             \u001b[0mtensor_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_val_mapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                                             tensor_Y: Y_val_mapped[start_index:end_index] })\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;31m# Statistics over the mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training data variables\n",
    "iterations_training = max((len(X_train) // batch_size), 1)\n",
    "print(\"Training iterations per epoch: \" + str(iterations_training))\n",
    "training_overfit = False\n",
    "consecutive_validation_without_saving = 0\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "iterations_validation = max((len(X_val_mapped) // batch_size), 1)\n",
    "\n",
    "# Initializer for variables in the graph\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Perform each epoch, shuffle training dataset\n",
    "indices = list(range(len(X_train)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables in the graph\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle data (with random seed for debug) to not train the network always with the same order\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_mapped = X_train_mapped[indices]\n",
    "        Y_train = Y_train[indices]\n",
    "        \n",
    "        # Vector accumulating accuracy and loss during for one epoch\n",
    "        total_accuracies, total_losses = [], []\n",
    "\n",
    "        # Iterate over mini-batches\n",
    "        for j in range(iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size \n",
    "            \n",
    "            # If last batch, take also elements that are less than batch_size\n",
    "            if j == (iterations_training - 1):\n",
    "                end_index += (batch_size - 1)\n",
    "\n",
    "            _, avg_accuracy, avg_loss = sess.run([optimizer, accuracy, loss], feed_dict={\n",
    "                                            tensor_X: X_train_mapped[start_index:end_index],\n",
    "                                            tensor_Y: Y_train[start_index:end_index] })\n",
    "            \n",
    "            # Add values for this mini-batch iterations\n",
    "            total_losses.append(avg_loss) \n",
    "            total_accuracies.append(avg_accuracy)\n",
    "\n",
    "            # Print loss and accuracy\n",
    "            if (j) % 100 == 0:\n",
    "                print(\"Loss: \" + str(avg_loss) + \", Accuracy: \" + str(avg_accuracy))\n",
    "                    \n",
    "            # Statistics on validation set\n",
    "            if (j) % 100 == 0:\n",
    "                \n",
    "                # Accumulate validation statistics\n",
    "                val_accuracies, val_losses = [], []\n",
    "\n",
    "                # Iterate over mini-batches\n",
    "                for k in range(iterations_validation):\n",
    "                    start_index = k * batch_size\n",
    "                    end_index = (k + 1) * batch_size \n",
    "                    \n",
    "                    if j == (iterations_validation - 1):\n",
    "                        end_index += (batch_size - 1)\n",
    "                        \n",
    "                    avg_accuracy, avg_loss = sess.run([accuracy, loss], feed_dict={\n",
    "                                            tensor_X: X_val_mapped[start_index:end_index],\n",
    "                                            tensor_Y: Y_val_mapped[start_index:end_index] })\n",
    "                    \n",
    "                    # Statistics over the mini-batch\n",
    "                    val_losses.append(avg_loss) \n",
    "                    val_accuracies.append(avg_accuracy)\n",
    "                    \n",
    "                # Average validation accuracy over batches\n",
    "                final_val_accuracy = np.mean(val_accuracies)\n",
    "                \n",
    "                # Save model if validation accuracy better\n",
    "                if final_val_accuracy > max_val_acc:\n",
    "                    consecutive_validation_without_saving = 0\n",
    "                    max_val_acc = final_val_accuracy\n",
    "                    print(\"VALIDATION loss: \" + str(np.mean(val_losses)) + \", accuracy: \" + str(final_val_accuracy))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")\n",
    "                else:\n",
    "                    # Count every time check validation accuracy\n",
    "                    consecutive_validation_without_saving += 1\n",
    "                \n",
    "                # If checked validation time many consecutive times without having improvement in accuracy\n",
    "                if consecutive_validation_without_saving >= 10:\n",
    "                    training_overfit = True\n",
    "                    #break\n",
    "                    \n",
    "        #if training_overfit:\n",
    "        #    print(\"Early stopping training because it starts overfitting\")\n",
    "        #    break\n",
    "            \n",
    "        # Epoch statistics\n",
    "        print(\"Training epoch: \" + str(i+1) + \", AVG loss: \" + str(np.mean(np.array(total_losses))) + \n",
    "              \", AVG accuracy: \" + str(np.mean(np.array(total_accuracies))) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# TF variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, \"./checkpoints/model.ckpt\") \n",
    "    \n",
    "    avg_accuracy, avg_loss, pred = sess.run([accuracy, loss, predictions], feed_dict={\n",
    "                                            tensor_X: X_val_mapped,\n",
    "                                            tensor_Y: Y_val_mapped })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "[0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_val_mapped[:50])\n",
    "print(pred[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :0.6403012116408108\n",
      "Precision :0.2791090111373608\n",
      "Recall :0.16395717684377478\n",
      "AUC :0.49733759935864447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "print(\"Accuracy :\" + str(accuracy_score(Y_val_mapped, pred)))\n",
    "print(\"Precision :\" + str(precision_score(Y_val_mapped, pred)))\n",
    "print(\"Recall :\" + str(recall_score(Y_val_mapped, pred)))\n",
    "print(\"AUC :\" + str(roc_auc_score(Y_val_mapped, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
